{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kFIRf0svB_qX",
        "KucYkEhJB6YM",
        "GrC66Q62FnSP",
        "UiO_a4DsI_Ry"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Textos de videos"
      ],
      "metadata": {
        "id": "kFIRf0svB_qX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalaciones necesarias\n",
        "!apt-get update\n",
        "!pip install selenium webdriver-manager flask youtube-transcript-api\n",
        "!pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lZR5oKsc5r3i",
        "outputId": "6f494b15-8e00-4c5b-a21e-67125551e62e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,901 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,920 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,245 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,282 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,717 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,545 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,436 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,211 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]\n",
            "Fetched 29.7 MB in 3s (8,528 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.32.0)\n",
            "Requirement already satisfied: webdriver-manager in /usr/local/lib/python3.11/dist-packages (4.0.2)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: youtube-transcript-api in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.4.0)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.4.26)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.13.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (2.32.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (24.2)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (3.4.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.16.0)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ruta origen\n",
        "url_videos = \"https://boardgamegeek.com/boardgame/199561/sagrada/videos/all?pageid=1&sort=hot\"\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "import time\n",
        "import csv\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Iniciar Selenium\n",
        "def start_driver(url, delay=5):\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    service = Service('/usr/bin/chromedriver')\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "    driver.get(url)\n",
        "    print(f\"Esperando {delay} segundos para que cargue la página...\")\n",
        "    time.sleep(delay)\n",
        "    return driver\n",
        "\n",
        "# Extraer vídeos\n",
        "def get_youtube_video_ids_from_bgg(driver, url_base):\n",
        "    driver.get(url_base)\n",
        "    time.sleep(5)\n",
        "    print(\"Cargando enlaces internos de videos...\")\n",
        "\n",
        "    # Buscar todos los enlaces a páginas de videos\n",
        "    links = driver.find_elements(By.CSS_SELECTOR, 'a[href^=\"/video/\"]')\n",
        "    video_page_urls = list(set([link.get_attribute(\"href\") for link in links]))\n",
        "\n",
        "    print(f\"Se encontraron {len(video_page_urls)} páginas de videos. Extrayendo enlaces de YouTube...\")\n",
        "\n",
        "    youtube_ids = []\n",
        "    for video_url in video_page_urls:\n",
        "        try:\n",
        "            driver.get(video_url)\n",
        "            time.sleep(3)\n",
        "            iframe = driver.find_element(By.CSS_SELECTOR, 'iframe[src*=\"youtube.com/embed\"]')\n",
        "            youtube_src = iframe.get_attribute('src')\n",
        "            video_id = youtube_src.split(\"/embed/\")[-1].split(\"?\")[0]\n",
        "            titulo = driver.title\n",
        "            youtube_ids.append((titulo, video_id, video_url))\n",
        "            print(f\"✔ {titulo} - ID: {video_id}\")\n",
        "        except Exception as e:\n",
        "            print(f\"✘ No se encontró video de YouTube en {video_url}: {e}\")\n",
        "    return youtube_ids\n",
        "\n",
        "\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from youtube_transcript_api._errors import TranscriptsDisabled, NoTranscriptFound\n",
        "from langdetect import detect\n",
        "\n",
        "def clean_filename(filename):\n",
        "    \"\"\"Limpia un string para usarlo como nombre de archivo\"\"\"\n",
        "    # Reemplazar caracteres no válidos para nombres de archivo\n",
        "    clean = re.sub(r'[\\\\/*?:\"<>|]', \"\", filename)\n",
        "    # Limitar la longitud para evitar problemas con rutas demasiado largas\n",
        "    if len(clean) > 150:\n",
        "        clean = clean[:150]\n",
        "    return clean\n",
        "\n",
        "def get_transcripts_to_separate_files(video_list, output_dir=\".\"):\n",
        "    \"\"\"Guarda cada transcripción en un archivo individual con el título del video\"\"\"\n",
        "    # Crear directorio si no existe\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    successful_transcripts = 0\n",
        "    failed_transcripts = 0\n",
        "\n",
        "    for titulo, video_id, url in video_list:\n",
        "        try:\n",
        "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['es', 'en'])\n",
        "            text = \"\\n\".join([seg[\"text\"] for seg in transcript])\n",
        "            detected_lang = detect(text)\n",
        "\n",
        "            # Crear un nombre de archivo seguro basado en el título\n",
        "            safe_title = clean_filename(titulo)\n",
        "            filename = f\"{output_dir}/{safe_title}.txt\"\n",
        "\n",
        "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(\n",
        "                    f\"TÍTULO: {titulo}\\n\"\n",
        "                    f\"ID: {video_id}\\n\"\n",
        "                    f\"URL: {url}\\n\"\n",
        "                    f\"IDIOMA DETECTADO: {detected_lang}\\n\"\n",
        "                    f\"TRANSCRIPCIÓN:\\n{text}\\n\"\n",
        "                )\n",
        "\n",
        "            successful_transcripts += 1\n",
        "            print(f\"✔ Transcripción guardada para video: {titulo} (Idioma: {detected_lang})\")\n",
        "            print(f\"  Archivo: {filename}\")\n",
        "\n",
        "        except (TranscriptsDisabled, NoTranscriptFound):\n",
        "            failed_transcripts += 1\n",
        "            print(f\"✘ No se pudo obtener transcripción para video: {titulo}\")\n",
        "        except Exception as e:\n",
        "            failed_transcripts += 1\n",
        "            print(f\"⚠ Error inesperado con el video {titulo}: {str(e)}\")\n",
        "\n",
        "    print(f\"\\nResumen: {successful_transcripts} transcripciones guardadas, {failed_transcripts} fallidas\")\n",
        "    return successful_transcripts, failed_transcripts"
      ],
      "metadata": {
        "id": "fdVDQJoR5rzs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutar todo\n",
        "driver = start_driver(url_videos)\n",
        "# Extraer todos los video_ids desde páginas internas\n",
        "video_info_list = get_youtube_video_ids_from_bgg(driver, url_videos)\n",
        "# Cerrar Selenium\n",
        "driver.quit()\n",
        "# Transcribir vídeos y guardarlos en archivos individuales\n",
        "output_directory = \"/content/transcripciones\"  # Directorio para guardar los archivos\n",
        "successful, failed = get_transcripts_to_separate_files(video_info_list, output_directory)\n",
        "print(f\"Proceso completado. Se guardaron {successful} transcripciones y fallaron {failed}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mopHwJszz_ih",
        "outputId": "ec30df62-d087-4a5d-ac2d-b71ac300990f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Esperando 5 segundos para que cargue la página...\n",
            "Cargando enlaces internos de videos...\n",
            "Se encontraron 37 páginas de videos. Extrayendo enlaces de YouTube...\n",
            "✔ Sagrada - A \"Chit\" Chat Review by Amanda and Amanda | Video | BoardGameGeek - ID: jLCThJFmWo4\n",
            "✔ The Art, Design, and Player Experience of Sagrada by Floodgate Games | Video | BoardGameGeek - ID: 4UUxnrQ2w14\n",
            "✔ Sagrada — Gen Con 2016 | Video | BoardGameGeek - ID: GMeu2GbirqI\n",
            "✔ Sagrada - Play Through, by Watch It Played | Video | BoardGameGeek - ID: w6mWCDnfLwc\n",
            "✔ JonGetsGames - Sagrada Full Playthrough | Video | BoardGameGeek - ID: 3dzhP7Ol_WY\n",
            "✔ Board to Death Video (6 min.) | Video | BoardGameGeek - ID: PvU9SuWU02I\n",
            "✔ Sagrada Review - with Tom Vasel | Video | BoardGameGeek - ID: kL-seAlTBW8\n",
            "✔ Bits of Board - Sagrada Review | Video | BoardGameGeek - ID: ib0etodHNS0\n",
            "✔ The Game Boy Geek's Allegro (2-min) Review of Sagrada | Video | BoardGameGeek - ID: iZJyGePr6c8\n",
            "✔ Bryan Drake @TheLatestRetro reviews Sagrada | Video | BoardGameGeek - ID: M86_eIRB0MI\n",
            "✔ The Gateway Guy reviews Sagrada | Video | BoardGameGeek - ID: NbIkubtxYRQ\n",
            "✔ Sagrada - Unboxing, tutorial y opinión - Juegos de mesa 221B | Video | BoardGameGeek - ID: zUx9w2ybPiU\n",
            "✔ Sagrada in about 3 minutes | Video | BoardGameGeek - ID: 2JchuN96jB4\n",
            "✔ Sagrada - unboxing and simplified play | Video | BoardGameGeek - ID: jG-D9P8tKV8\n",
            "✔ Gen Con Bonanza 2016: Sagrada Interview | Video | BoardGameGeek - ID: FRQEEpFyRoM\n",
            "✔ solomode games - Sagrada solo session | Video | BoardGameGeek - ID: 0jW26nL5zFQ\n",
            "✔ Sagrada: Unboxing | Video | BoardGameGeek - ID: Gv-IA4IcgsM\n",
            "✔ Overview from Gen Con 2017 | Video | BoardGameGeek - ID: A90j2ALmIfo\n",
            "✔ Sagrada - Playthrough | Video | BoardGameGeek - ID: v_3g9vMiq5w\n",
            "✔ Video Review | Video | BoardGameGeek - ID: N6Kow5ZAYWY\n",
            "✔ Sagrada [Juego de Mesa / Como se Juega / Tutorial] | Video | BoardGameGeek - ID: rAXn5VAHznk\n",
            "✔ The Game Boy Geek Reviews Sagrada | Video | BoardGameGeek - ID: zdxmGpisRg8\n",
            "✔ Rahdo Runs Through►►► Sagrada | Video | BoardGameGeek - ID: 3BSonqR6RX0\n",
            "✔ [DriveThruReview] #586: “Die Burgund Von Basílica” | Video | BoardGameGeek - ID: nsJqrQOsFoA\n",
            "✘ No se encontró video de YouTube en https://boardgamegeek.com/video/new/thing/199561: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"iframe[src*=\"youtube.com/embed\"]\"}\n",
            "  (Session info: chrome=136.0.7103.92); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
            "Stacktrace:\n",
            "#0 0x560aaaea278a <unknown>\n",
            "#1 0x560aaa9450a0 <unknown>\n",
            "#2 0x560aaa9969b0 <unknown>\n",
            "#3 0x560aaa996ba1 <unknown>\n",
            "#4 0x560aaa9e4ea4 <unknown>\n",
            "#5 0x560aaa9bc3cd <unknown>\n",
            "#6 0x560aaa9e22a0 <unknown>\n",
            "#7 0x560aaa9bc173 <unknown>\n",
            "#8 0x560aaa988d4b <unknown>\n",
            "#9 0x560aaa9899b1 <unknown>\n",
            "#10 0x560aaae6793b <unknown>\n",
            "#11 0x560aaae6b83a <unknown>\n",
            "#12 0x560aaae4f692 <unknown>\n",
            "#13 0x560aaae6c3c4 <unknown>\n",
            "#14 0x560aaae344cf <unknown>\n",
            "#15 0x560aaae90568 <unknown>\n",
            "#16 0x560aaae90746 <unknown>\n",
            "#17 0x560aaaea15f6 <unknown>\n",
            "#18 0x7e0a0f389ac3 <unknown>\n",
            "\n",
            "✔ Sagrada Review by Man Vs Meeple | Video | BoardGameGeek - ID: olAPPUvVlBE\n",
            "✔ Sagrada Overview + Solo Playthrough | Video | BoardGameGeek - ID: 2hJ8y_o9P38\n",
            "✔ The Score reviews Sagrada! | Video | BoardGameGeek - ID: F2J_i_tk4ro\n",
            "✔ Sagrada - GameNight! Se5 Ep14 | Video | BoardGameGeek - ID: EPcQmGsuyzw\n",
            "✔ Boardgame Opinions: Sagrada | Video | BoardGameGeek - ID: aoW-Ow3xkZc\n",
            "✔ Sagrada - Unboxing | Video | BoardGameGeek - ID: eQOGVm1AvK0\n",
            "✔ Sagrada - How To Play, by Watch It Played | Video | BoardGameGeek - ID: 0JLpaGHL8MQ\n",
            "✔ [NTFG] Sagrada - Stained Glass | Video | BoardGameGeek - ID: FD_Eo22tplA\n",
            "✔ Never Bored Gaming - Our Thoughts | Video | BoardGameGeek - ID: R8v2yzZNpNc\n",
            "✔ Edo' Sagrada Review | Video | BoardGameGeek - ID: xXZKamFXmNs\n",
            "✔ One Stop Solo Playthrough | Video | BoardGameGeek - ID: v5Ldgz7UFoE\n",
            "✔ Sagrada Review | Video | BoardGameGeek - ID: I8OVtFrGnUg\n",
            "✔ Transcripción guardada para video: Sagrada - A \"Chit\" Chat Review by Amanda and Amanda | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/Sagrada - A Chit Chat Review by Amanda and Amanda  Video  BoardGameGeek.txt\n",
            "✔ Transcripción guardada para video: The Art, Design, and Player Experience of Sagrada by Floodgate Games | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/The Art, Design, and Player Experience of Sagrada by Floodgate Games  Video  BoardGameGeek.txt\n",
            "✔ Transcripción guardada para video: Sagrada — Gen Con 2016 | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/Sagrada — Gen Con 2016  Video  BoardGameGeek.txt\n",
            "✔ Transcripción guardada para video: Sagrada - Play Through, by Watch It Played | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/Sagrada - Play Through, by Watch It Played  Video  BoardGameGeek.txt\n",
            "✔ Transcripción guardada para video: JonGetsGames - Sagrada Full Playthrough | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/JonGetsGames - Sagrada Full Playthrough  Video  BoardGameGeek.txt\n",
            "✔ Transcripción guardada para video: Board to Death Video (6 min.) | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/Board to Death Video (6 min.)  Video  BoardGameGeek.txt\n",
            "✔ Transcripción guardada para video: Sagrada Review - with Tom Vasel | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/Sagrada Review - with Tom Vasel  Video  BoardGameGeek.txt\n",
            "✔ Transcripción guardada para video: Bits of Board - Sagrada Review | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/Bits of Board - Sagrada Review  Video  BoardGameGeek.txt\n",
            "✔ Transcripción guardada para video: The Game Boy Geek's Allegro (2-min) Review of Sagrada | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/The Game Boy Geek's Allegro (2-min) Review of Sagrada  Video  BoardGameGeek.txt\n",
            "✔ Transcripción guardada para video: Bryan Drake @TheLatestRetro reviews Sagrada | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/Bryan Drake @TheLatestRetro reviews Sagrada  Video  BoardGameGeek.txt\n",
            "✔ Transcripción guardada para video: The Gateway Guy reviews Sagrada | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/The Gateway Guy reviews Sagrada  Video  BoardGameGeek.txt\n",
            "✘ No se pudo obtener transcripción para video: Sagrada - Unboxing, tutorial y opinión - Juegos de mesa 221B | Video | BoardGameGeek\n",
            "✔ Transcripción guardada para video: Sagrada in about 3 minutes | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/Sagrada in about 3 minutes  Video  BoardGameGeek.txt\n",
            "✔ Transcripción guardada para video: Sagrada - unboxing and simplified play | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/Sagrada - unboxing and simplified play  Video  BoardGameGeek.txt\n",
            "✔ Transcripción guardada para video: Gen Con Bonanza 2016: Sagrada Interview | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/Gen Con Bonanza 2016 Sagrada Interview  Video  BoardGameGeek.txt\n",
            "✔ Transcripción guardada para video: solomode games - Sagrada solo session | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/solomode games - Sagrada solo session  Video  BoardGameGeek.txt\n",
            "⚠ Error inesperado con el video Sagrada: Unboxing | Video | BoardGameGeek: no element found: line 1, column 0\n",
            "✘ No se pudo obtener transcripción para video: Overview from Gen Con 2017 | Video | BoardGameGeek\n",
            "✔ Transcripción guardada para video: Sagrada - Playthrough | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/Sagrada - Playthrough  Video  BoardGameGeek.txt\n",
            "✔ Transcripción guardada para video: Video Review | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/Video Review  Video  BoardGameGeek.txt\n",
            "⚠ Error inesperado con el video Sagrada [Juego de Mesa / Como se Juega / Tutorial] | Video | BoardGameGeek: no element found: line 1, column 0\n",
            "✔ Transcripción guardada para video: The Game Boy Geek Reviews Sagrada | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/The Game Boy Geek Reviews Sagrada  Video  BoardGameGeek.txt\n",
            "✔ Transcripción guardada para video: Rahdo Runs Through►►► Sagrada | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/Rahdo Runs Through►►► Sagrada  Video  BoardGameGeek.txt\n",
            "⚠ Error inesperado con el video [DriveThruReview] #586: “Die Burgund Von Basílica” | Video | BoardGameGeek: no element found: line 1, column 0\n",
            "✔ Transcripción guardada para video: Sagrada Review by Man Vs Meeple | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/Sagrada Review by Man Vs Meeple  Video  BoardGameGeek.txt\n",
            "✘ No se pudo obtener transcripción para video: Sagrada Overview + Solo Playthrough | Video | BoardGameGeek\n",
            "⚠ Error inesperado con el video The Score reviews Sagrada! | Video | BoardGameGeek: \n",
            "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=F2J_i_tk4ro! This is most likely caused by:\n",
            "\n",
            "The video is no longer available\n",
            "\n",
            "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
            "⚠ Error inesperado con el video Sagrada - GameNight! Se5 Ep14 | Video | BoardGameGeek: no element found: line 1, column 0\n",
            "✔ Transcripción guardada para video: Boardgame Opinions: Sagrada | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/Boardgame Opinions Sagrada  Video  BoardGameGeek.txt\n",
            "✔ Transcripción guardada para video: Sagrada - Unboxing | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/Sagrada - Unboxing  Video  BoardGameGeek.txt\n",
            "✔ Transcripción guardada para video: Sagrada - How To Play, by Watch It Played | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/Sagrada - How To Play, by Watch It Played  Video  BoardGameGeek.txt\n",
            "✔ Transcripción guardada para video: [NTFG] Sagrada - Stained Glass | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/[NTFG] Sagrada - Stained Glass  Video  BoardGameGeek.txt\n",
            "✔ Transcripción guardada para video: Never Bored Gaming - Our Thoughts | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/Never Bored Gaming - Our Thoughts  Video  BoardGameGeek.txt\n",
            "✔ Transcripción guardada para video: Edo' Sagrada Review | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/Edo' Sagrada Review  Video  BoardGameGeek.txt\n",
            "✔ Transcripción guardada para video: One Stop Solo Playthrough | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/One Stop Solo Playthrough  Video  BoardGameGeek.txt\n",
            "✔ Transcripción guardada para video: Sagrada Review | Video | BoardGameGeek (Idioma: en)\n",
            "  Archivo: /content/transcripciones/Sagrada Review  Video  BoardGameGeek.txt\n",
            "\n",
            "Resumen: 28 transcripciones guardadas, 8 fallidas\n",
            "Proceso completado. Se guardaron 28 transcripciones y fallaron 8.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Textos de pdf y docx"
      ],
      "metadata": {
        "id": "KucYkEhJB6YM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber\n",
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cxGOg-lSBymO",
        "outputId": "48cde1dd-755a-41c1-c48f-b7efa2fb90e8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Using cached pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
            "Collecting pdfminer.six==20250327 (from pdfplumber)\n",
            "  Using cached pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.2.1)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Using cached pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n",
            "Using cached pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
            "Using cached pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
            "Using cached pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "Installing collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20250327 pdfplumber-0.11.6 pypdfium2-4.30.1\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Directorio de salida (mismo que para las transcripciones de video)\n",
        "output_directory = \"/content/transcripciones\"\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "# Lista de URLs de GitHub\n",
        "github_urls = [\n",
        "    \"https://github.com/GrimaldiDamian/sagrada/blob/main/codigo/docx_pdfs/SAGRADA.pdf\",\n",
        "    \"https://github.com/GrimaldiDamian/sagrada/blob/main/codigo/docx_pdfs/Sagrada-Rules-Floodgate-Games-SA01.pdf\",\n",
        "    \"https://github.com/GrimaldiDamian/sagrada/blob/main/codigo/docx_pdfs/Sagrada__Automa_wPassion_-Deluxe_-_German.pdf\",\n",
        "    \"https://github.com/GrimaldiDamian/sagrada/blob/main/codigo/docx_pdfs/Sagrada_solitaire_variant.docx\"\n",
        "]\n",
        "\n",
        "import requests\n",
        "import io\n",
        "import pdfplumber\n",
        "from docx import Document\n",
        "\n",
        "def clean_filename(filename):\n",
        "    \"\"\"Limpia un string para usarlo como nombre de archivo\"\"\"\n",
        "    # Reemplazar caracteres no válidos para nombres de archivo\n",
        "    clean = re.sub(r'[\\\\/*?:\"<>|]', \"\", filename)\n",
        "    # Limitar la longitud para evitar problemas con rutas demasiado largas\n",
        "    if len(clean) > 150:\n",
        "        clean = clean[:150]\n",
        "    return clean\n",
        "\n",
        "def get_raw_content_url(github_url):\n",
        "    \"\"\"Convierte una URL de GitHub en la URL de contenido raw\"\"\"\n",
        "    # Convertir la URL de GitHub a la URL de raw content\n",
        "    raw_url = github_url.replace(\"github.com\", \"raw.githubusercontent.com\")\n",
        "    raw_url = raw_url.replace(\"/blob/\", \"/\")\n",
        "    return raw_url\n",
        "\n",
        "def download_file(url):\n",
        "    \"\"\"Descarga un archivo desde una URL y devuelve su contenido como bytes\"\"\"\n",
        "    raw_url = get_raw_content_url(url)\n",
        "    print(f\"Descargando archivo desde: {raw_url}\")\n",
        "    response = requests.get(raw_url)\n",
        "    if response.status_code == 200:\n",
        "        return response.content\n",
        "    else:\n",
        "        print(f\"Error al descargar el archivo: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "def extraer_texto_pdf(contenido_bytes):\n",
        "    \"\"\"Extrae texto de un PDF a partir de sus bytes\"\"\"\n",
        "    texto = \"\"\n",
        "    try:\n",
        "        with pdfplumber.open(io.BytesIO(contenido_bytes)) as pdf:\n",
        "            for pagina in pdf.pages:\n",
        "                extracted_text = pagina.extract_text()\n",
        "                if extracted_text:\n",
        "                    texto += extracted_text + \"\\n\"\n",
        "        return texto\n",
        "    except Exception as e:\n",
        "        print(f\"Error al procesar PDF: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def extraer_texto_docx(contenido_bytes):\n",
        "    \"\"\"Extrae texto de un DOCX a partir de sus bytes\"\"\"\n",
        "    try:\n",
        "        doc = Document(io.BytesIO(contenido_bytes))\n",
        "        return \"\\n\".join([p.text for p in doc.paragraphs])\n",
        "    except Exception as e:\n",
        "        print(f\"Error al procesar DOCX: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def procesar_archivos():\n",
        "    \"\"\"Procesa cada archivo de la lista y guarda su contenido en un archivo de texto\"\"\"\n",
        "    successful = 0\n",
        "    failed = 0\n",
        "\n",
        "    for url in github_urls:\n",
        "        # Obtener el nombre del archivo de la URL\n",
        "        filename = url.split(\"/\")[-1]\n",
        "        print(f\"\\nProcesando: {filename}\")\n",
        "\n",
        "        try:\n",
        "            # Descargar el archivo\n",
        "            contenido = download_file(url)\n",
        "            if not contenido:\n",
        "                print(f\"✘ No se pudo descargar: {filename}\")\n",
        "                failed += 1\n",
        "                continue\n",
        "\n",
        "            # Extraer texto según el tipo de archivo\n",
        "            if filename.lower().endswith(\".pdf\"):\n",
        "                texto = extraer_texto_pdf(contenido)\n",
        "            elif filename.lower().endswith(\".docx\"):\n",
        "                texto = extraer_texto_docx(contenido)\n",
        "            else:\n",
        "                print(f\"✘ Formato no soportado: {filename}\")\n",
        "                failed += 1\n",
        "                continue\n",
        "\n",
        "            if not texto:\n",
        "                print(f\"✘ No se pudo extraer texto de: {filename}\")\n",
        "                failed += 1\n",
        "                continue\n",
        "\n",
        "            # Crear nombre de archivo de salida\n",
        "            name_without_extension = os.path.splitext(filename)[0]\n",
        "            output_filename = clean_filename(name_without_extension) + \".txt\"\n",
        "            output_path = os.path.join(output_directory, output_filename)\n",
        "\n",
        "            # Guardar texto en archivo individual\n",
        "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(f\"ARCHIVO ORIGINAL: {filename}\\n\")\n",
        "                f.write(f\"URL: {url}\\n\")\n",
        "                f.write(f\"CONTENIDO:\\n\\n{texto}\\n\")\n",
        "\n",
        "            successful += 1\n",
        "            print(f\"✔ Texto extraído y guardado: {output_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Error inesperado con {filename}: {str(e)}\")\n",
        "            failed += 1\n",
        "\n",
        "    print(f\"\\nResumen: {successful} archivos procesados correctamente, {failed} fallidos\")\n",
        "    return successful, failed\n"
      ],
      "metadata": {
        "id": "zuVdpP9mBe21"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutar el procesamiento\n",
        "print(\"Iniciando procesamiento de archivos desde GitHub...\")\n",
        "successful, failed = procesar_archivos()\n",
        "print(f\"Proceso completado. Se procesaron {successful} archivos y fallaron {failed}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvEY1czRCMsS",
        "outputId": "cefb09dd-3b6f-47f8-c3cb-8ef47fac10b9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando procesamiento de archivos desde GitHub...\n",
            "\n",
            "Procesando: SAGRADA.pdf\n",
            "Descargando archivo desde: https://raw.githubusercontent.com/GrimaldiDamian/sagrada/main/codigo/docx_pdfs/SAGRADA.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Texto extraído y guardado: /content/transcripciones/SAGRADA.txt\n",
            "\n",
            "Procesando: Sagrada-Rules-Floodgate-Games-SA01.pdf\n",
            "Descargando archivo desde: https://raw.githubusercontent.com/GrimaldiDamian/sagrada/main/codigo/docx_pdfs/Sagrada-Rules-Floodgate-Games-SA01.pdf\n",
            "✔ Texto extraído y guardado: /content/transcripciones/Sagrada-Rules-Floodgate-Games-SA01.txt\n",
            "\n",
            "Procesando: Sagrada__Automa_wPassion_-Deluxe_-_German.pdf\n",
            "Descargando archivo desde: https://raw.githubusercontent.com/GrimaldiDamian/sagrada/main/codigo/docx_pdfs/Sagrada__Automa_wPassion_-Deluxe_-_German.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Texto extraído y guardado: /content/transcripciones/Sagrada__Automa_wPassion_-Deluxe_-_German.txt\n",
            "\n",
            "Procesando: Sagrada_solitaire_variant.docx\n",
            "Descargando archivo desde: https://raw.githubusercontent.com/GrimaldiDamian/sagrada/main/codigo/docx_pdfs/Sagrada_solitaire_variant.docx\n",
            "✔ Texto extraído y guardado: /content/transcripciones/Sagrada_solitaire_variant.txt\n",
            "\n",
            "Resumen: 4 archivos procesados correctamente, 0 fallidos\n",
            "Proceso completado. Se procesaron 4 archivos y fallaron 0.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Textos de página misutmeeple"
      ],
      "metadata": {
        "id": "GrC66Q62FnSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "def clean_filename(filename):\n",
        "    \"\"\"Limpia un string para usarlo como nombre de archivo\"\"\"\n",
        "    # Reemplazar caracteres no válidos para nombres de archivo\n",
        "    clean = re.sub(r'[\\\\/*?:\"<>|]', \"\", filename)\n",
        "    # Limitar la longitud para evitar problemas con rutas demasiado largas\n",
        "    if len(clean) > 150:\n",
        "        clean = clean[:150]\n",
        "    return clean\n",
        "\n",
        "def extraer_contenido_resena(url):\n",
        "    \"\"\"\n",
        "    Extrae contenido estructurado de una reseña de juego de mesa.\n",
        "    Devuelve una lista con líneas de texto extraídas y un diccionario con las URLs de imágenes.\n",
        "    \"\"\"\n",
        "    resultados = []  # Lista para acumular el contenido extraído\n",
        "    imagenes_urls = []  # Lista para acumular URLs de imágenes\n",
        "\n",
        "    # 1. Obtener el HTML de la página\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            html_content = response.text\n",
        "            resultados.append(\"✅ Página obtenida con éxito\\n\")\n",
        "        else:\n",
        "            resultados.append(f\"❌ Error al acceder a la página: {response.status_code}\\n\")\n",
        "            return resultados, imagenes_urls\n",
        "    except Exception as e:\n",
        "        resultados.append(f\"❌ Error al acceder a la página: {str(e)}\\n\")\n",
        "        return resultados, imagenes_urls\n",
        "\n",
        "    # 2. Crear objeto BeautifulSoup\n",
        "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "    # 3. Título y encabezados\n",
        "    try:\n",
        "        titulo = soup.find(\"title\").text.strip()\n",
        "        resultados.append(f\"Título de la pestaña: {titulo}\\n\")\n",
        "    except:\n",
        "        resultados.append(\"❌ No se encontró el título de la página\\n\")\n",
        "        titulo = \"sin_titulo\"\n",
        "\n",
        "    try:\n",
        "        heading = soup.find(\"h1\").text.strip()\n",
        "        resultados.append(f\"H1: {heading}\\n\")\n",
        "    except:\n",
        "        resultados.append(\"❌ No se encontró el encabezado H1\\n\")\n",
        "\n",
        "    for nivel, tag in zip([\"H2\", \"H3\", \"H4\"], [\"h2\", \"h3\", \"h4\"]):\n",
        "        encabezados = soup.find_all(tag)\n",
        "        if encabezados:\n",
        "            resultados.append(f\"\\n{nivel} encontrados:\\n\")\n",
        "            for i in encabezados:\n",
        "                resultados.append(f\"- {i.text.strip()}\")\n",
        "\n",
        "    # 4. Párrafos\n",
        "    parrafos = soup.find_all(\"p\")\n",
        "    if parrafos:\n",
        "        resultados.append(\"\\n\\nPárrafos:\\n\")\n",
        "        for i, parrafo in enumerate(parrafos, 1):\n",
        "            texto = parrafo.text.strip()\n",
        "            if texto:\n",
        "                resultados.append(f\"{i}. {texto}\")\n",
        "\n",
        "    # 5. Palabras en negrita\n",
        "    strongs = soup.find_all(\"strong\")\n",
        "    if strongs:\n",
        "        resultados.append(\"\\n\\nPalabras en negrita:\\n\")\n",
        "        for i in strongs:\n",
        "            texto = i.text.strip()\n",
        "            if texto:\n",
        "                resultados.append(f\"- {texto}\")\n",
        "\n",
        "    # 6. Imágenes\n",
        "    resultados.append(\"\\n\\nURLs de Imágenes encontradas:\\n\")\n",
        "    try:\n",
        "        content_wrap = soup.find(class_=\"entry-content-wrap\") or soup.find(\"article\") or soup\n",
        "        imagenes = content_wrap.find_all(\"img\")\n",
        "        for i, img in enumerate(imagenes, 1):\n",
        "            img_url = img.get(\"src\")\n",
        "            if img_url:\n",
        "                if img_url.startswith(\"/\"):\n",
        "                    parsed_url = urlparse(url)\n",
        "                    base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "                    img_url = base_url + img_url\n",
        "                resultados.append(f\"Imagen {i}: {img_url}\")\n",
        "                imagenes_urls.append((f\"imagen_{i}.jpg\", img_url))\n",
        "    except Exception as e:\n",
        "        resultados.append(f\"❌ Error al extraer imágenes: {str(e)}\\n\")\n",
        "\n",
        "    # 7. Comentarios\n",
        "    comentarios = soup.select(\".comment-body\")\n",
        "    if comentarios:\n",
        "        resultados.append(\"\\n\\nComentarios:\\n\")\n",
        "        for i, comentario in enumerate(comentarios, 1):\n",
        "            try:\n",
        "                autor = comentario.find(class_=\"fn\").text.strip()\n",
        "                texto = comentario.find(class_=\"comment-content\").text.strip()\n",
        "                resultados.append(f\"{i}. AUTOR: {autor}\\nCOMENTARIO: {texto}\\n\")\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    # 8. Listas\n",
        "    elementos_lista = soup.find_all(\"ul\") + soup.find_all(\"ol\")\n",
        "    if elementos_lista:\n",
        "        resultados.append(\"\\n\\nElementos de listas:\\n\")\n",
        "        for ul in elementos_lista:\n",
        "            lista_li = ul.find_all(\"li\")\n",
        "            for li in lista_li:\n",
        "                resultados.append(f\"- {li.text.strip()}\")\n",
        "\n",
        "    # 9. Leyendas de imágenes\n",
        "    leyendas = soup.find_all(\"figcaption\")\n",
        "    if leyendas:\n",
        "        resultados.append(\"\\n\\nLeyendas de imágenes:\\n\")\n",
        "        for i in leyendas:\n",
        "            texto = i.text.strip()\n",
        "            if texto:\n",
        "                resultados.append(f\"- {texto}\")\n",
        "\n",
        "    return resultados, imagenes_urls, titulo\n",
        "\n",
        "def procesar_multiples_urls(urls, output_directory=\"/content/transcripciones\"):\n",
        "    \"\"\"\n",
        "    Procesa múltiples URLs, extrae su contenido y guarda cada una en un archivo de texto separado.\n",
        "    También descarga las imágenes asociadas a cada URL en carpetas dedicadas.\n",
        "    \"\"\"\n",
        "    # Crear directorio de salida si no existe\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "    successful = 0\n",
        "    failed = 0\n",
        "\n",
        "    for url in urls:\n",
        "        print(f\"\\nProcesando URL: {url}\")\n",
        "        try:\n",
        "            # Extraer contenido\n",
        "            contenido, imagenes_urls, titulo = extraer_contenido_resena(url)\n",
        "\n",
        "            if not contenido:\n",
        "                print(f\"✘ No se pudo extraer contenido de: {url}\")\n",
        "                failed += 1\n",
        "                continue\n",
        "\n",
        "            # Crear nombre de archivo para esta URL\n",
        "            domain = urlparse(url).netloc.replace(\"www.\", \"\")\n",
        "            path = urlparse(url).path.strip(\"/\").replace(\"/\", \"_\")\n",
        "            if path == \"\":\n",
        "                path = \"home\"\n",
        "\n",
        "            safe_filename = clean_filename(f\"{domain}_{path}\")\n",
        "            txt_filename = f\"{safe_filename}.txt\"\n",
        "            output_path = os.path.join(output_directory, txt_filename)\n",
        "\n",
        "            # Guardar contenido en archivo\n",
        "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(f\"URL ORIGINAL: {url}\\n\")\n",
        "                f.write(f\"TÍTULO: {titulo}\\n\\n\")\n",
        "                f.write(\"\\n\".join(contenido))\n",
        "\n",
        "            # Crear carpeta para imágenes si hay alguna\n",
        "            if imagenes_urls:\n",
        "                img_folder = os.path.join(output_directory, f\"imagenes_{safe_filename}\")\n",
        "                os.makedirs(img_folder, exist_ok=True)\n",
        "\n",
        "                # Descargar imágenes (limitado a las primeras 5)\n",
        "                for i, (img_name, img_url) in enumerate(imagenes_urls[:5], 1):\n",
        "                    try:\n",
        "                        print(f\"  Descargando imagen {i}/{min(5, len(imagenes_urls))}: {img_url}\")\n",
        "                        img_data = requests.get(img_url, timeout=10).content\n",
        "                        img_path = os.path.join(img_folder, img_name)\n",
        "                        with open(img_path, \"wb\") as f:\n",
        "                            f.write(img_data)\n",
        "                        # Pequeña pausa\n",
        "                        time.sleep(1)\n",
        "                    except Exception as e:\n",
        "                        print(f\"  ✘ Error al descargar imagen {i}: {str(e)}\")\n",
        "\n",
        "            successful += 1\n",
        "            print(f\"✔ Contenido extraído y guardado: {output_path}\")\n",
        "            if imagenes_urls:\n",
        "                print(f\"✔ Imágenes guardadas en: {img_folder}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Error inesperado con {url}: {str(e)}\")\n",
        "            failed += 1\n",
        "\n",
        "    print(f\"\\nResumen: {successful} URLs procesadas correctamente, {failed} fallidas\")\n",
        "    return successful, failed"
      ],
      "metadata": {
        "id": "kJrhQ--MG9Ur"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de URLs a procesar\n",
        "urls_a_procesar = [\n",
        "    \"https://misutmeeple.com/2018/08/resena-sagrada/\"\n",
        "]\n",
        "\n",
        "output_directory = \"/content/transcripciones\"\n",
        "\n",
        "# Ejecutar el script\n",
        "print(\"Iniciando el scraping de múltiples URLs de reseñas de juegos...\")\n",
        "successful, failed = procesar_multiples_urls(urls_a_procesar, output_directory)\n",
        "print(f\"Proceso completado. Se procesaron {successful} URLs y fallaron {failed}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SkWiDSXG9Sh",
        "outputId": "91edc957-fb14-45a6-df15-cde034a85c52"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando el scraping de múltiples URLs de reseñas de juegos...\n",
            "\n",
            "Procesando URL: https://misutmeeple.com/2018/08/resena-sagrada/\n",
            "  Descargando imagen 1/5: https://i0.wp.com/misutmeeple.com/wp-content/uploads/2018/08/sagrada_portada.jpg?resize=1200%2C801&ssl=1\n",
            "  Descargando imagen 2/5: https://i0.wp.com/misutmeeple.com/wp-content/uploads/2018/08/sagrada_contraportada.jpg?resize=1200%2C801&ssl=1\n",
            "  Descargando imagen 3/5: https://i0.wp.com/misutmeeple.com/wp-content/uploads/2018/08/sagrada_contenido.jpg?resize=1200%2C394&ssl=1\n",
            "  Descargando imagen 4/5: https://i0.wp.com/misutmeeple.com/wp-content/uploads/2018/08/sagrada_dados.jpg?resize=1200%2C246&ssl=1\n",
            "  Descargando imagen 5/5: https://i0.wp.com/misutmeeple.com/wp-content/uploads/2018/08/sagrada_tablero_vidriera.jpg?resize=1200%2C823&ssl=1\n",
            "✔ Contenido extraído y guardado: /content/transcripciones/misutmeeple.com_2018_08_resena-sagrada.txt\n",
            "✔ Imágenes guardadas en: /content/transcripciones/imagenes_misutmeeple.com_2018_08_resena-sagrada\n",
            "\n",
            "Resumen: 1 URLs procesadas correctamente, 0 fallidas\n",
            "Proceso completado. Se procesaron 1 URLs y fallaron 0.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estadísticas"
      ],
      "metadata": {
        "id": "UiO_a4DsI_Ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def start_driver(url: str, delay: int = 5) -> webdriver.Chrome:\n",
        "    options = Options()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    options.add_argument('--disable-gpu')\n",
        "\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    driver.get(url)\n",
        "    time.sleep(delay)\n",
        "    return driver\n",
        "\n",
        "def guardar_csv(df: pd.DataFrame, output_dir: str, nombre_base: str):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    path_csv = os.path.join(output_dir, f\"{nombre_base}.csv\")\n",
        "    df.to_csv(path_csv, sep=';', index=False)\n",
        "    print(f\"✅ CSV guardado: {path_csv}\")\n",
        "\n",
        "def obtener_stats(url: str, delay: int = 5) -> pd.DataFrame:\n",
        "    driver = None\n",
        "    try:\n",
        "        print(f\"Accediendo a {url}...\")\n",
        "        driver = start_driver(url, delay)\n",
        "\n",
        "        stats = driver.find_element(By.CLASS_NAME, \"global-body-content-primary.ng-scope\")\n",
        "        titulos = stats.find_elements(By.CLASS_NAME, \"panel-title\")\n",
        "        titulos = [t for t in titulos if t.tag_name == \"h3\"]\n",
        "        titulos_texto = [t.text for t in titulos]\n",
        "        paneles = stats.find_elements(By.CLASS_NAME, \"panel-body\")\n",
        "\n",
        "        datos = {}\n",
        "        for titulo, panel in zip(titulos_texto, paneles):\n",
        "            if titulo != \"RATINGS BREAKDOWN\":\n",
        "                items = panel.find_elements(By.CLASS_NAME, \"outline-item\")\n",
        "                valores = [item.text.replace(\"\\n\", \" \") for item in items]\n",
        "                datos[titulo] = valores\n",
        "\n",
        "        return pd.DataFrame({k: pd.Series(v) for k, v in datos.items()})\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {e}\")\n",
        "        return pd.DataFrame()\n",
        "    finally:\n",
        "        if driver:\n",
        "            driver.quit()\n",
        "\n",
        "def procesar_estadisticas(urls_dict, output_dir=\"/content/estadisticas\"):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    for nombre_juego, url in urls_dict.items():\n",
        "        print(f\"\\nProcesando: {nombre_juego}\")\n",
        "        df = obtener_stats(url)\n",
        "        if not df.empty:\n",
        "            guardar_csv(df, output_dir, f\"estadisticas_{nombre_juego.lower().replace(' ', '_')}\")\n",
        "        else:\n",
        "            print(f\"❌ No se pudieron obtener estadísticas para {nombre_juego}\")"
      ],
      "metadata": {
        "id": "vpYRWMDSNB43"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Diccionario con URLs\n",
        "urls_estadisticas = {\n",
        "    \"Sagrada\": \"https://boardgamegeek.com/boardgame/199561/sagrada/stats\",\n",
        "    \"Sagrada_5-6_Player\": \"https://boardgamegeek.com/boardgameexpansion/232199/sagrada-5-6-player-expansion/stats\",\n",
        "    \"Sagrada_Great_Facades\": \"https://boardgamegeek.com/boardgameexpansion/293768/sagrada-great-facades-passion/stats\"\n",
        "}\n",
        "\n",
        "# Ejecutar\n",
        "print(\"Iniciando extracción...\")\n",
        "procesar_estadisticas(urls_estadisticas)\n",
        "print(\"Proceso finalizado.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sc4iKxKpKsSM",
        "outputId": "f63cc700-8e68-46f6-f03f-55c98889f961"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando extracción...\n",
            "\n",
            "Procesando: Sagrada\n",
            "Accediendo a https://boardgamegeek.com/boardgame/199561/sagrada/stats...\n",
            "✅ CSV guardado: /content/estadisticas/estadisticas_sagrada.csv\n",
            "\n",
            "Procesando: Sagrada_5-6_Player\n",
            "Accediendo a https://boardgamegeek.com/boardgameexpansion/232199/sagrada-5-6-player-expansion/stats...\n",
            "✅ CSV guardado: /content/estadisticas/estadisticas_sagrada_5-6_player.csv\n",
            "\n",
            "Procesando: Sagrada_Great_Facades\n",
            "Accediendo a https://boardgamegeek.com/boardgameexpansion/293768/sagrada-great-facades-passion/stats...\n",
            "✅ CSV guardado: /content/estadisticas/estadisticas_sagrada_great_facades.csv\n",
            "Proceso finalizado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7Xf9C4MgWjRe"
      }
    }
  ]
}